{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOML_ASSIGNMENT_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Name: ANTALA AVIRAJ (CS24MTECH14011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Question 5_a) Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Logistic (sigmoid) function to compute probabilities\n",
    "def logistic_function(x, theta):\n",
    "    z = np.dot(x, theta)  # Compute linear combination\n",
    "    return 1 / (1 + np.exp(-z))  # Apply sigmoid function\n",
    "\n",
    "# Cross-entropy error calculation\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-10  # Small value to avoid numerical instability\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)  # Clip predictions to avoid log(0)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Gradient descent optimization\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations=1000, tolerance=1e-6):\n",
    "    m = len(y)  # Number of samples\n",
    "    for _ in range(iterations):\n",
    "        y_pred = logistic_function(X, theta)  # Get predictions\n",
    "        error = y_pred - y  # Calculate residuals\n",
    "        gradient = np.dot(X.T, error) / m  # Compute gradient\n",
    "        theta_new = theta - learning_rate * gradient  # Update parameters\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(theta_new - theta, ord=2) < tolerance:\n",
    "            theta = theta_new\n",
    "            break\n",
    "        theta = theta_new\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Question 5_b_i) Logistic Regression::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Logistic Model:\n",
    "$$\n",
    "P(\\hat{y} = 1 \\mid x_1, x_2) = \\frac{1}{1 + \\exp\\left(-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)\\right)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Cross-Entropy Error Function:\n",
    "$$\n",
    "E(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log\\left(P(\\hat{y}_i)\\right) + (1 - y_i) \\log\\left(1 - P(\\hat{y}_i)\\right) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Question 5_b_ii) Logistic Regression::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Theta after one iteration:\n",
      "[-1.00316626  1.50535086  0.50196867]\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_data = {\n",
    "    'x1': [0.346, 0.303, 0.358, 0.602, 0.790, 0.611],\n",
    "    'x2': [0.780, 0.439, 0.729, 0.863, 0.753, 0.965],\n",
    "    'y': [0, 0, 0, 1, 1, 1]\n",
    "}\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "# Initial weights and learning rate\n",
    "theta = np.array([-1, 1.5, 0.5])  # Starting values for model parameters\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Prepare training data\n",
    "X_train = np.c_[np.ones(len(train_df)), train_df[['x1', 'x2']].values]  # Combine bias term with features\n",
    "y_train = train_df['y'].values\n",
    "\n",
    "# Compute predictions and gradient for one iteration of gradient descent\n",
    "y_pred_train = logistic_function(X_train, theta)  # Predictions using current theta\n",
    "error = y_pred_train - y_train  # Residual error\n",
    "gradient = np.dot(X_train.T, error) / len(y_train)  # Gradient of loss with respect to theta\n",
    "theta_updated = theta - learning_rate * gradient  # Update theta using gradient descent rule\n",
    "\n",
    "# Print updated weights\n",
    "print(\"Updated Theta after one iteration:\")\n",
    "print(theta_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "       Evaluation Metrics Report       \n",
      "===================================\n",
      "Metric                        Value\n",
      "-----------------------------------\n",
      "Accuracy                      66.67%\n",
      "Precision                      0.60\n",
      "Recall                         1.00\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "# Test dataset\n",
    "test_data = {\n",
    "    'x1': [0.959, 0.750, 0.395, 0.823, 0.761, 0.844],\n",
    "    'x2': [0.382, 0.306, 0.760, 0.764, 0.874, 0.435],\n",
    "    'y': [0, 0, 0, 1, 1, 1]\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Train to convergence\n",
    "theta_converged = gradient_descent(X_train, y_train, theta, learning_rate)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = np.c_[np.ones(len(test_df)), test_df[['x1', 'x2']].values]  # Combine bias term with features\n",
    "y_test = test_df['y'].values\n",
    "\n",
    "# Make predictions using the converged model\n",
    "y_pred_test = (logistic_function(X_test, theta_converged) >= 0.5)  # Predicted class labels\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = np.mean(y_pred_test == y_test) * 100  # Accuracy as a percentage\n",
    "precision = np.sum((y_pred_test == 1) & (y_test == 1)) / np.sum(y_pred_test == 1) if np.sum(y_pred_test == 1) > 0 else 0\n",
    "recall = np.sum((y_pred_test == 1) & (y_test == 1)) / np.sum(y_test == 1) if np.sum(y_test == 1) > 0 else 0\n",
    "\n",
    "# Print evaluation metrics in a clean format\n",
    "print(\"=\"*35)\n",
    "print(\"       Evaluation Metrics Report       \")\n",
    "print(\"=\"*35)\n",
    "print(f\"{'Metric':<15}{'Value':>20}\")\n",
    "print(\"-\"*35)\n",
    "print(f\"{'Accuracy':<15}{accuracy:>20.2f}%\")\n",
    "print(f\"{'Precision':<15}{precision:>20.2f}\")\n",
    "print(f\"{'Recall':<15}{recall:>20.2f}\")\n",
    "print(\"=\"*35)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
