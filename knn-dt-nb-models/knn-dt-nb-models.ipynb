{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4] Decision Trees:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SUBJECT: Foundations of Machine Learning(CS5590)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    NAME: Aviraj Antala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "#  calculating  entropy function\n",
    "def entropy(y):\n",
    "    labels = np.unique(y) #let's say two diff. lable yes and no then take both unique value\n",
    "    entropy_value = 0\n",
    "    for label in labels:\n",
    "        p = sum(y == label) / len(y) # for yes is yes then sum and divid by whole length\n",
    "        entropy_value -= p * np.log2(p)\n",
    "    return entropy_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculating Gini index\n",
    "def gini(y):\n",
    "    labels = np.unique(y)\n",
    "    gini_value = 1\n",
    "    for label in labels:\n",
    "        p = sum(y == label) / len(y)\n",
    "        gini_value -= p ** 2 #same as entropy just formula is different \n",
    "    return gini_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculating information gain using entropy or Gini\n",
    "def information_gain(y, X_column, threshold, criterion='entropy'): # here y is target column #x_column is feature column \n",
    "    if criterion == 'entropy':\n",
    "        parent_impurity = entropy(y) #for parents we calculate entropy\n",
    "    else:\n",
    "        parent_impurity = gini(y) #for parents we calculate gini\n",
    "    \n",
    "    left_indices = X_column <= threshold # we divie them into split based on threshold let's say threshold is 5 then 5<= left, \n",
    "    right_indices = X_column > threshold #other in right\n",
    "    \n",
    "    if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "        return 0        #if one class is not present in the node then it is pure node so entropy become 0.\n",
    "    \n",
    "    n = len(y)\n",
    "    n_left, n_right = sum(left_indices), sum(right_indices) #for calculating weighted probability\n",
    "    \n",
    "    if criterion == 'entropy':\n",
    "        e_left = entropy(y[left_indices]) \n",
    "        e_right = entropy(y[right_indices])\n",
    "    else:\n",
    "        e_left = gini(y[left_indices])\n",
    "        e_right = gini(y[right_indices])\n",
    "    \n",
    "    child_impurity = (n_left / n) * e_left + (n_right / n) * e_right \n",
    "    \n",
    "    return parent_impurity - child_impurity #final information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best split\n",
    "def best_split(X, y, criterion='entropy'):\n",
    "    best_gain = -1\n",
    "    best_feature = None #starting best features is NONE\n",
    "    best_threshold = None # same for threshold\n",
    "    \n",
    "    for feature_index in range(X.shape[1]): #x.shape[1] means first row which contain features\n",
    "        X_column = X[:, feature_index] # all row of that particular features let's consider fixed acidity here\n",
    "        thresholds = np.unique(X_column) #all unique value of fixed acidity\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            gain = information_gain(y, X_column, threshold, criterion) #calculate IG for all unique threshold and for feature also\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature_index #assign best features \n",
    "                best_threshold = threshold # assign best threshold\n",
    "    \n",
    "    return best_feature, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the decision tree\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion \n",
    "        self.tree = None #intialize tree \n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        # Set label at the leaf\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return self._most_common_label(y) #this if is ude for let's say node have only one value or if depth >max_depth \n",
    "                                              #then give label to that node ex: yes or no\n",
    "        feature_index, threshold = best_split(X, y, self.criterion)\n",
    "        \n",
    "        if feature_index is None: #if there is no any feature remain based on that we split then give label to node\n",
    "            return self._most_common_label(y)\n",
    "        \n",
    "        left_indices = X[:, feature_index] <= threshold\n",
    "        right_indices = X[:, feature_index] > threshold\n",
    "        \n",
    "        left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1) #recursion for construct tree\n",
    "        right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return {\"feature_index\": feature_index, \"threshold\": threshold,\n",
    "                \"left\": left_subtree, \"right\": right_subtree} #it will generate dictionary type node in to the trees\n",
    "    \n",
    "    def predict(self, X): # now let's say we have whole decision tree and particular instance x coming for test\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X]) #then we traverse to the tree\n",
    "    \n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if isinstance(tree, dict): #that particular instance is dictionary type or not\n",
    "            feature_index = tree[\"feature_index\"]\n",
    "            threshold = tree[\"threshold\"]\n",
    "            if x[feature_index] <= threshold: #comparing with threshold if <= go into left subtree\n",
    "                return self._traverse_tree(x, tree[\"left\"])\n",
    "            else:\n",
    "                return self._traverse_tree(x, tree[\"right\"])\n",
    "        else:                       #if not that means it is leaf node where label is present\n",
    "            return tree\n",
    "    \n",
    "    def _most_common_label(self, y): #function for assigning label\n",
    "        labels, counts = np.unique(y, return_counts=True)\n",
    "        return labels[np.argmax(counts)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation\n",
    "def k_fold_cross_validation(X, y, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42) #we can split it into 10 parts\n",
    "    accuracies = [] #we ar making one list to count final mean acuracy \n",
    "    \n",
    "    for train_index, test_index in kf.split(X):# it will take index of 9 parts \n",
    "        X_train, X_test = X[train_index], X[test_index] #assign it to x_train and one assign to X_test\n",
    "        y_train, y_test = y[train_index], y[test_index] #also take target value for final vertification\n",
    "        \n",
    "        clf.tree = clf.fit(X_train, y_train) #creating whole tree\n",
    "        predictions = clf.predict(X_test) #give data for prediction\n",
    "        accuracy = np.sum(predictions == y_test) / len(y_test) #verify with original value\n",
    "        accuracies.append(accuracy) # do for all 10 parts and append it to the accuracy list\n",
    "    \n",
    "    return np.mean(accuracies) #create final list and take mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-pruning function (basic implementation)\n",
    "def post_prune_tree(tree, X_val, y_val, clf): #original tree and validation set pass to here\n",
    "    if not isinstance(tree, dict): #if leaf node then\n",
    "        return tree\n",
    "    \n",
    "    left_subtree = post_prune_tree(tree['left'], X_val, y_val, clf)  #goes to lastnode or left bottom node for pruning\n",
    "    right_subtree = post_prune_tree(tree['right'], X_val, y_val, clf)\n",
    "    \n",
    "    if isinstance(left_subtree, dict) or isinstance(right_subtree, dict):\n",
    "        return tree\n",
    "    # here I use this method for pruning we can also do pruning based on error i comment that code also but this function give me more accuracy.\n",
    "    # merge nodes if both subtrees are leaves and the same\n",
    "    combined_label = clf._most_common_label(np.concatenate([y_val[X_val[:, tree['feature_index']] <= tree['threshold']],\n",
    "                                                            y_val[X_val[:, tree['feature_index']] > tree['threshold']]]))\n",
    "    \n",
    "    if (combined_label == left_subtree and combined_label == right_subtree):\n",
    "        return combined_label\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here I use this method for pruning we can also do pruning based on error I comment that code also but this function give me more accuracy.\n",
    "\n",
    "* In this we can also use alpha that say that don't wait for to pure node take some error rate if error less than this \n",
    "* remove the node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def post_prune_tree(tree, X_val, y_val, clf):\n",
    "#     # node is a leaf node return \n",
    "#     if not isinstance(tree, dict):\n",
    "#         return tree\n",
    "    \n",
    "#     # go till last node\n",
    "#     left_subtree = post_prune_tree(tree['left'], X_val, y_val, clf)\n",
    "#     right_subtree = post_prune_tree(tree['right'], X_val, y_val, clf)\n",
    "\n",
    "#     \n",
    "#     if isinstance(left_subtree, dict) or isinstance(right_subtree, dict):\n",
    "#         return tree\n",
    "\n",
    "#     check whether pruning is beneficial\n",
    "#    \n",
    "#     left_indices = X_val[:, tree['feature_index']] <= tree['threshold']\n",
    "#     right_indices = X_val[:, tree['feature_index']] > tree['threshold']\n",
    "    \n",
    "#     Predictions of left and right nodes\n",
    "#     left_predictions = np.full(np.sum(left_indices), left_subtree)\n",
    "#     right_predictions = np.full(np.sum(right_indices), right_subtree)\n",
    "\n",
    "#     calculate the error without pruning (ex:current node)\n",
    "#     val_predictions = np.concatenate([left_predictions, right_predictions])\n",
    "#     val_labels = np.concatenate([y_val[left_indices], y_val[right_indices]])\n",
    "    \n",
    "#     current_error = np.mean(val_predictions != val_labels) \n",
    "\n",
    "#     # replace with single node\n",
    "#     combined_label = clf._most_common_label(val_labels)\n",
    "#     combined_error = np.mean(combined_label != val_labels) \n",
    "\n",
    "#     # replace node with leaf\n",
    "#     if combined_error <= current_error:\n",
    "#         return combined_label\n",
    "\n",
    "#     # else keep the current split\n",
    "#     return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can enter whole data also using (red wine ,white wine) \n",
    "wine_data = fetch_ucirepo(id=186) \n",
    "* X = wine_data.data.features.to_numpy() \n",
    "* y = wine_data.data.targets  || it is mention on that website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for decision tree with entropy\n",
      "Enter 2 for decision tree with 10-fold cross-validation using entropy.\n",
      "Enter 3 for decision tree using Gini index using K-Foled cross-validation.\n",
      "Enter 4 for decision tree with post-pruning using K-Fold cross-validation.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_data = pd.read_csv(url, sep=';')\n",
    "X = wine_data.iloc[:, :-1].values # we take all the row of colum from 0 to upto -1\n",
    "y = wine_data.iloc[:, -1].values #we take all the row of last column\n",
    "\n",
    "# Convert to binary classification (0/1)\n",
    "y = (y >= 7).astype(int) #in question it is mention that if quality >= 7 then it is 1 else 0\n",
    "\n",
    "# now let's try to do all in one codey = (y >= 7).astype(int) #in question it is mention that if quality >= 7 then it is 1 else 0\n",
    "\n",
    "print(\"Enter 1 for decision tree with entropy\")\n",
    "print(\"Enter 2 for decision tree with 10-fold cross-validation using entropy.\")\n",
    "print(\"Enter 3 for decision tree using Gini index using K-Foled cross-validation.\")\n",
    "print(\"Enter 4 for decision tree with post-pruning using K-Fold cross-validation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for decision tree with entropy\n",
      "Enter 2 for decision tree with 10-fold cross-validation using entropy.\n",
      "Enter 3 for decision tree using Gini index using K-Foled cross-validation.\n",
      "Enter 4 for decision tree with post-pruning using K-Fold cross-validation.\n",
      "Accuracy with gini index decision tree with 10 k-fold: 0.889308176100629\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter 1 for decision tree with entropy\")\n",
    "print(\"Enter 2 for decision tree with 10-fold cross-validation using entropy.\")\n",
    "print(\"Enter 3 for decision tree using Gini index using K-Foled cross-validation.\")\n",
    "print(\"Enter 4 for decision tree with post-pruning using K-Fold cross-validation.\")\n",
    "\n",
    "choice = int(input())\n",
    "if choice == 1:\n",
    "    # Case 1:for decision tree with entropy\n",
    "    clf = DecisionTree(max_depth=10, criterion='entropy')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    clf.tree = clf.fit(X_train, y_train) # create decision tree based on taining\n",
    "    predictions = clf.predict(X_test) #predict in testing data\n",
    "    accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "    print(f'Accuracy using entropy-based decision tree: {accuracy}')\n",
    "    #y = (y >= 7).astype(int) #in question it is mention that if quality >= 7 then it is 1 else 0\n",
    "\n",
    "\n",
    "elif choice == 2:\n",
    "    # Case 2: for decision tree with 10-fold cross-validation using entropy\n",
    "    clf = DecisionTree(max_depth=10, criterion='entropy')\n",
    "    accuracy = k_fold_cross_validation(X, y, clf, k=10)\n",
    "    print(f'Accuracy using 10-fold cross-validation with entropy: {accuracy}')\n",
    "\n",
    "elif choice == 3:\n",
    "    # Case 3: for decision tree using Gini index using K-Foled cross-validation\n",
    "    clf = DecisionTree(max_depth=10, criterion='gini')\n",
    "    accuracy = k_fold_cross_validation(X, y, clf, k=10)\n",
    "    print(f'Accuracy with gini index decision tree with 10 k-fold: {accuracy}')\n",
    "\n",
    "elif choice == 4:\n",
    "    # Case 4: for decision tree with post-pruning using k-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index] #first it doing 10 partition for k-fold (9 fold for training and 1 for testing)\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # split training set into training and validation for pruning\n",
    "        split_index = int(0.8 * len(X_train)) #now here we seperate 20% of data for validation out of training and take 80% for training\n",
    "        X_train_part, X_val = X_train[:split_index], X_train[split_index:] # starting index to particular split index goes into X_train_part and split_index to end goes into validation\n",
    "        y_train_part, y_val = y_train[:split_index], y_train[split_index:]\n",
    "        \n",
    "        # train the tree and apply post-pruning\n",
    "        clf = DecisionTree(max_depth=6, criterion='entropy')\n",
    "        clf.tree = clf.fit(X_train_part, y_train_part) #built tree using training part\n",
    "        pruned_tree = post_prune_tree(clf.tree, X_val, y_val, clf) #checking error into validation set and pass original tree\n",
    "        clf.tree = pruned_tree #final pruned tree\n",
    "        \n",
    "        predictions = clf.predict(X_test)\n",
    "        accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    print(f'accuracy with post-pruning usign k-fold: {average_accuracy}')\n",
    "\n",
    "else:\n",
    "    print(\"invalid choice.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer A] Decision Tree Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I implement the whole decision tree using entropy function and gini function, also I take help from various references like bishop and youtube for developing intuition.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when I press 1 it gives me accuracy approx 0.8 to 0.9.\n",
    "* Accuracy is pretty high because I classify my whole data into 0 and 1 class using y = (y >= 7).astype(int)\n",
    "* so if I run code without converting it into 2 classes it gives me accuracy approx 0.5 to 0.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer B] Cross Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of using k-fold implementation:0.873687106918239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First I use KFold method then take one accuracy list after that pass 9 parts(folds) and then verify with other 10th part(folds) <br>\n",
    "* We need to that for 10 times. <br>\n",
    "* Then take to mean of that accuracy list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer C] Improvement Strategies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1] Use Gini index instead of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of intial implementation:0.873687106918239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy after gini implementation:0.889308176100629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using gini index accuracy increase slightly but it doesn't show that much improvement <br>\n",
    "* because whether gini gives better results or entropy depends on a particular dataset. <br>\n",
    "* In this case if I don't classify data into class 0 and class 1 then entropy gives better results.(without K-fold) <br>\n",
    "* but if I use k fold with gini then it increases slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2] Prune the tree after splitting for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of intial implementation:0.873687106918239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy after pruning :0.889308176100629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Post pruning technique is used for reducing overfitting in decision trees.<br>\n",
    " * but it is implemented after a full tree is grown so we can say that it does not give that much good accuracy.<br> \n",
    " * also it require extra time for post pruning <br> \n",
    " * But it helps to simplify the tree using removing the node that is not that valuable for the result.<br> \n",
    " * In this particular case accuracy is not improving that much if I use it with k-fold verification. <br>  In the case of pruning accuracy also depends on the validation set.<br>  \n",
    " * It also depends on the max_depth factor if i use max_depth as 10 or 8 then also accuracy will be affected <br> \n",
    " * So based on this we can say that for this particular case accuracy will be increased but that doesn't create that much huge difference. <br> \n",
    " * For doing post pruning we can use multiple method like let's say if 2 node are same then combine it <br> another method is error method: here we first come to bottom leaf node calculate error for validation    set if this error is less than error on training set then we remove this node. <br> \n",
    " * for more modification we can use alhpha also which don't wait for node be pure <br> \n",
    " * we set particular value of alhpha and if error is less than this we can remove this <br> \n",
    " * I tried with all three method but maximum accuracy I got into above code so I use this "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
